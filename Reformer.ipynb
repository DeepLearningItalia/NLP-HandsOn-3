{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1687c4f7-34ab-4dd6-89f4-140e735108f1",
   "metadata": {},
   "source": [
    "### Reformer\n",
    "\n",
    "Dopo aver installato le dipendenze con: \n",
    "- conda create -n nlp-hands-on-3 python=3.8\n",
    "- pip install -r requirements.txt\n",
    "\n",
    "Vediamo in azione il nostro Reformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc2a553-7ef6-4e35-8e2f-c1708b75abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reformer_pytorch import ReformerLM\n",
    "from reformer_pytorch.generative_tools import TrainingWrapper\n",
    "from reformer_pytorch import ReformerEncDec\n",
    "\n",
    "import random\n",
    "import tqdm\n",
    "import gzip\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71a03648-14ac-4945-9d5e-8c64df5bbfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "NUM_BATCHES = int(1e5)\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATE_EVERY = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "VALIDATE_EVERY  = 100\n",
    "GENERATE_EVERY  = 500\n",
    "GENERATE_LENGTH = 512\n",
    "SEQ_LEN = 4096\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d44ce08-3f74-4dcc-bf7a-5465d265f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def decode_token(token):\n",
    "    return str(chr(max(32, token)))\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return ''.join(list(map(decode_token, tokens)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a9eaeb5-2c3f-46ea-84d5-9967ea5e6ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "\n",
    "model = ReformerLM(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    num_tokens = 256,\n",
    "    heads = 8,\n",
    "    bucket_size = 64,\n",
    "    n_hashes = 4,\n",
    "    ff_chunks = 10,\n",
    "    lsh_dropout = 0.1,\n",
    "    weight_tie = True,\n",
    "    causal = True,\n",
    "    n_local_attn_heads = 4,\n",
    "    use_full_attn = False # set this to true for comparison with full attention\n",
    ")\n",
    "\n",
    "model = TrainingWrapper(model)\n",
    "#model.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6fe1af4-fc56-44e6-9710-7fe8a13d8f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34354/2752233459.py:4: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  X = np.fromstring(file.read(int(95e6)), dtype=np.uint8)\n"
     ]
    }
   ],
   "source": [
    "# prepare enwik8 data\n",
    "\n",
    "with gzip.open('./data/enwik8.gz') as file:\n",
    "    X = np.fromstring(file.read(int(95e6)), dtype=np.uint8)\n",
    "    trX, vaX = np.split(X, [int(90e6)])\n",
    "    data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)\n",
    "\n",
    "class TextSamplerDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len - 1, (1,))\n",
    "        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n",
    "        return full_seq#.cuda()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) // self.seq_len\n",
    "\n",
    "train_dataset = TextSamplerDataset(data_train, SEQ_LEN)\n",
    "val_dataset   = TextSamplerDataset(data_val, SEQ_LEN)\n",
    "train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\n",
    "val_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2f183fd-2175-40ce-aa27-4ad2764b38ed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                     | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 5.712687969207764\n",
      "validation loss: 5.393490314483643\n",
      "%s \n",
      "\n",
      " %s (' Database and HFR Industry Reports] *[http://icf.som.yale.edu/research/hedgefund.shtml Hedge Fund Research Initiative] of the International Center for Finance at the [[Yale School of Management]] *[http://www.hedgefund.net/HFN_Averages_January_06_Report.pdf/ HFN Averages January Performance Report] *[http://www.hedgefund.net/Strategy_Focus_Report_022806.pdf/ HedgeFund.net Strategy Focus Report: HFN Small/Micro Cap Average]  {{finance-footer}}  [[category:Funds]]  [[de:Hedge-Fonds]] [[fr:Gestion alternative]] [[ja:&amp;#12504;&amp;#12483;&amp;#12472;&amp;#12501;&amp;#12449;&amp;#12531;&amp;#12489;]] [[zh:å¯¹å\\x86²å\\x9fºé\\x87\\x91]]</text>     </revision>   </page>   <page>     <title>Hydrocodone</title>     <id>14413</id>     <revision>       <id>42147808</id>       <timestamp>2006-03-04T03:21:19Z</timestamp>       <contributor>         <username>Shanel</username>         <id>301280</id>       </contributor>       <minor />       <comment>Reverted edits by [[Special:Contributions/64.108.220.245|64.108.220.245]] ([[User talk:64.108.220.245|talk]]) to last version by Cjewell</comment>       <text xml:space=\"preserve\">{| class=&quot;infobox bordered&quot; cellpadding=&quot;3&quot; cellspacing=&quot;0&quot; width=&quot;250px&quot; |- | bgcolor=&quot;#ffffff&quot; align=&quot;center&quot; colspan=&quot;2&quot; | [[Image:Hydrocodone.png|Hydrocodone chemical structure]]&lt;br/&gt; \\'\\'Hydrocodone\\'\\' |- | align=&quot;center&quot; colspan=&quot;2&quot; | \\'\\'4,5a-Epoxy-3-methoxy-17-methylmorphinan-6-one tartrate (1:1) hydrate (2:5)\\'\\' |- align=&quot;center&quot; style=&quot;border-bottom: 3px solid gray&quot; | \\'\\'\\'[[CAS number]]\\'\\'\\' &lt;br/&gt; 125-29-1 | \\'\\'\\'[[ATC code]]\\'\\'\\'   &lt;br/&gt; R05DA03 |- | bgcolor=&quot;#eeeeee&quot; | [[Chemical formula]] | bgcolor=&quot;#ddeeff&quot; | C&lt;sub&gt;18&lt;/sub&gt;H&lt;sub&gt;21&lt;/sub&gt;NO&lt;sub&gt;3&lt;/sub&gt; |- | bgcolor=&quot;#eeeeee&quot; | [[Molecular weight]] | bgcolor=&quot;#ddeeff&quot; | 299.368 |- | bgcolor=&quot;#eeeeee&quot; | [[Bioavailability]] | bgcolor=&quot;#ddeeff&quot; | ? |- | bgcolor=&quot;#eeeeee&quot; | Metabolism | bgcolor=&quot;#ddeeff&quot; | Hepatic |- | bgcolor=&quot;#eeeeee&quot; | [[Elimination half-life]] | bgcolor=&quot;#ddeeff&quot; | 4â\\x80\\x938 hours |- | bgcolor=&quot;#eeeeee&quot; | [[Excretion]] | bgcolor=&quot;#ddeeff&quot; | Renal |- | bgcolor=&quot;#eeeeee&quot; | [[Pregnancy category (pharmaceutical)|Pregnancy category]]  | bgcolor=&quot;#ddeeff&quot; | Category C (USA) |- | bgcolor=&quot;#eeeeee&quot; | [[Regulation of therapeutic goods|Legal status]] | bgcolor=&quot;#ddeeff&quot; | [[Misuse of Drugs Act 1971|Class A]] (UK), [[Controlled Substances Act|Schedule II/III]] (USA)  |- | bgcolor=&quot;#eeeeee&quot; | Routes of administration | bgcolor=&quot;#ddeeff&quot; | Oral |- |} \\'\\'\\'Hydrocodone\\'\\'\\' or \\'\\'\\'dihydrocodeinone\\'\\'\\' (marketed as \\'\\'\\'Vicodin\\'\\'\\', \\'\\'\\'Anexsia\\'\\'\\', \\'\\'\\'Dicodid\\'\\'\\', \\'\\'\\'Hycodan\\'\\'\\', \\'\\'\\'Hycomine\\'\\'\\', \\'\\'\\'Lorcet\\'\\'\\', \\'\\'\\'Lortab\\'\\'\\', \\'\\'\\'Norco\\'\\'\\', \\'\\'\\'Tussionex\\'\\'\\', \\'\\'\\'Vicoprofen\\'\\'\\') is an [[opioid]] derived from either of the naturally occurring [[opiates]] [[codeine]] or [[thebaine]]. Hydrocodone is an orally active [[narcotic]] [[analgesic]] and [[antitussive]]. The typical therapeutic dose of 5 to 10 [[Milligram|mg]] is [[Pharmacology|pharmacologically]] equivalent to 30 to 60 mg of oral [[codeine]].{{ref|tarascon}} Sales and production of this [[medication|drug]] have increased significantly in recent years, as have diversion and illicit use.  Hydrocodone is commonly available in tablet, capsule and syrup form.   As a [[narcotic]], hydrocodone relieves [[pain]] by binding to [[Opioid receptor|opioid receptors]] in the [[brain]] and [[spinal cord]].  It may be taken with or without food, but should never be combined with alcohol.  It may interact with [[monoamine oxidase inhibitors]], as well as other drugs that cause drowsiness.  It is in [[Food and Drug Administration | FDA]] [[pregnancy category]] C:  its effect on an unborn [[embryo]] or [[fetus]] is not clearly known and pregnant women should consult their physicians before taking it.  Common [[Adverse drug', '****************************************************************************************************')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                     | 1/100000 [16:42<27843:56:04, 1002.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ºµhïäK|ÆÖ   ¯'® =oY  é^'<C5 Çü}ñsÏX:Ï côuñþ¦  ®o+n\"æÏ{%±éxx¯'®®®  0Ñá ã©ÄêÂÑÿ    x  Fm^¹òt Ê º 6à]uÖ ÙtÖ¸xÂ33í× äRXÔ ½v{'cáè0¹òsýLÞÊ ½á=Ää¸e%o0c (R¡nvÐ  ShaÍs38ÿäf e´ ¥W|°Â®xÂÖûrá 5GV:QhbFv{>o  7 ÅÁ8à ø-ºîéèNøÈ¢tjhlæE=?QkC ¾ ¤f éþ»  KÊÂ3 ®ß¹¬þ2ÚÖ] ei¯¾äsn %÷u Ñ K|GXèÐ7#5àÒXnád ¤m}$¡nr®oY _7¿°i  7 ~?¼sWàÑáÂéxQ Ü^Ye~u­ fÄöçÂÔ¿i³ÜS³÷ µ Î=9Nì~g«%ÏËýLÎ",
      "ÏÊ%¦<{2ïèÖeF8 o0U\\0~Và÷ cK WÄêªeæ _¯Ý(õµÖÌ²¸e[½Å©ªmïX ýdLþÊg]% sÏ ½;  }cXSoÓó/sW+E  K×g°X·C ÓóÂo©\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                      | 2/100000 [18:58<13693:17:37, 492.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 5.367817401885986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                       | 3/100000 [21:13<9151:57:14, 329.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 4.965239524841309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                       | 4/100000 [23:28<7019:08:04, 252.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 4.744425296783447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                       | 5/100000 [25:43<5835:31:36, 210.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 4.450093746185303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                       | 6/100000 [27:56<5113:25:02, 184.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 4.1302666664123535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                       | 7/100000 [30:12<4670:46:10, 168.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 4.087907791137695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                       | 8/100000 [32:26<4372:49:29, 157.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.7613089084625244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                       | 9/100000 [34:43<4192:36:02, 150.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.7313404083251953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                      | 10/100000 [36:56<4040:06:04, 145.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.635711669921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                      | 11/100000 [39:10<3944:32:41, 142.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.606860876083374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                      | 12/100000 [41:27<3896:48:41, 140.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.6015586853027344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                      | 13/100000 [43:46<3889:23:15, 140.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.5999655723571777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                      | 14/100000 [46:07<3896:12:07, 140.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.521846294403076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                      | 15/100000 [48:25<3881:57:20, 139.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.5089263916015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                      | 16/100000 [50:45<3879:41:20, 139.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.380735158920288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                      | 17/100000 [53:05<3879:22:19, 139.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.2193524837493896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                      | 18/100000 [55:24<3877:12:49, 139.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.167842388153076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                      | 19/100000 [57:44<3877:35:12, 139.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.2234718799591064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 20/100000 [1:00:03<3873:05:08, 139.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.238429307937622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 21/100000 [1:02:23<3877:51:44, 139.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.3115792274475098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 22/100000 [1:04:42<3875:30:30, 139.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.2402753829956055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 23/100000 [1:07:02<3879:40:47, 139.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.3628571033477783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 24/100000 [1:09:23<3886:12:58, 139.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.3242132663726807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 25/100000 [1:11:42<3876:27:40, 139.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.1888089179992676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 26/100000 [1:14:00<3869:47:05, 139.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.3380186557769775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 27/100000 [1:16:20<3874:32:03, 139.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.503314256668091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 28/100000 [1:18:40<3879:00:35, 139.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.2019283771514893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 29/100000 [1:21:01<3883:24:08, 139.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.08240008354187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 30/100000 [1:23:20<3878:31:02, 139.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.000864267349243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 31/100000 [1:25:38<3869:06:05, 139.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.0344667434692383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 32/100000 [1:27:57<3859:51:12, 139.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.973862409591675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 33/100000 [1:30:18<3876:48:38, 139.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.9405009746551514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 34/100000 [1:32:39<3888:30:18, 140.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.9729928970336914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 35/100000 [1:34:59<3890:33:46, 140.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.010070323944092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 36/100000 [1:37:19<3892:41:05, 140.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.2033281326293945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 37/100000 [1:39:39<3888:53:18, 140.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.9056718349456787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 38/100000 [1:41:55<3852:56:46, 138.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.812208890914917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 39/100000 [1:44:09<3812:59:28, 137.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.9044740200042725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 40/100000 [1:46:23<3787:15:25, 136.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8990018367767334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 41/100000 [1:48:39<3784:15:39, 136.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8606436252593994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 42/100000 [1:50:54<3777:31:09, 136.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.040532350540161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 43/100000 [1:53:10<3770:21:51, 135.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.940197706222534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 44/100000 [1:55:24<3760:46:17, 135.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8159096240997314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 45/100000 [1:57:39<3754:37:59, 135.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8694612979888916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 46/100000 [1:59:53<3745:45:49, 134.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.848743438720703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 47/100000 [2:02:07<3737:04:09, 134.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.881574869155884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 48/100000 [2:04:22<3744:02:01, 134.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.826660633087158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 49/100000 [2:06:38<3745:22:02, 134.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8716864585876465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 50/100000 [2:08:54<3761:24:56, 135.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8646767139434814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 51/100000 [2:11:12<3777:38:55, 136.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8590614795684814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 52/100000 [2:13:28<3781:08:32, 136.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.9650213718414307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                    | 52/100000 [2:15:02<4326:11:56, 155.82s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34354/2272006128.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRADIENT_ACCUMULATE_EVERY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'training loss: {loss.item()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-3/lib/python3.8/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# _forward_cls is defined by derived class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-3/lib/python3.8/site-packages/reformer_pytorch/reversible.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, dy)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-3/lib/python3.8/site-packages/reformer_pytorch/reversible.py\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfx2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfx2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mdx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdy2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# training\n",
    "\n",
    "for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='training'):\n",
    "    model.train()\n",
    "\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        loss = model(next(train_loader), return_loss = True)\n",
    "        loss.backward()\n",
    "\n",
    "    print(f'training loss: {loss.item()}')\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss = model(next(val_loader), return_loss = True)\n",
    "            print(f'validation loss: {loss.item()}')\n",
    "\n",
    "    if i % GENERATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        inp = random.choice(val_dataset)[:-1]\n",
    "        prime = decode_tokens(inp)\n",
    "        print(f'%s \\n\\n %s', (prime, '*' * 100))\n",
    "\n",
    "        sample = model.generate(inp, GENERATE_LENGTH)\n",
    "        output_str = decode_tokens(sample)\n",
    "        print(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb01049-a2f0-466f-9824-dce7805d2d8d",
   "metadata": {},
   "source": [
    "### More models\n",
    "\n",
    "Adesso che abbiamo visto un semplice Language Model, passiamo a qualcosa di più interessante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306e531-1564-41c6-9323-0ffd4804d9e3",
   "metadata": {},
   "source": [
    "## Translation 1\n",
    "Sequence to Sequence - Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3074c25a-4cdb-4537-b181-1188a45bec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from reformer_pytorch import ReformerLM\n",
    "\n",
    "DE_SEQ_LEN = 2048\n",
    "EN_SEQ_LEN = 2048\n",
    "\n",
    "encoder_engine = ReformerLM(\n",
    "    num_tokens = 10000,#20000,\n",
    "    emb_dim = 128,\n",
    "    dim = 256, # 1024,\n",
    "    depth = 6, #12,\n",
    "    heads = 4, #8,\n",
    "    max_seq_len = DE_SEQ_LEN,\n",
    "    fixed_position_emb = True,\n",
    "    return_embeddings = True # return output of last attention layer\n",
    ")#.cuda()\n",
    "\n",
    "decoder_engine = ReformerLM(\n",
    "    num_tokens = 10000,#20000,\n",
    "    emb_dim = 128,\n",
    "    dim = 256, # 1024,\n",
    "    depth = 6, #12,\n",
    "    heads = 4, #8,\n",
    "    max_seq_len = EN_SEQ_LEN,\n",
    "    fixed_position_emb = True,\n",
    "    causal = True\n",
    ")#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4862f0d9-aa81-41a5-9d35-60907c2025b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                    | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: -711421.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:45<00:00, 45.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation Loss: -1059510.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Creiamo il dataset e facciamo il train!\n",
    "enc_optim = torch.optim.Adam(encoder_engine.parameters(), lr=LEARNING_RATE)\n",
    "dec_optim = torch.optim.Adam(decoder_engine.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "validate_every = 100\n",
    "for tr_step in tqdm.tqdm(range(1000)):\n",
    "    ## Sostituire questo con i token del vostro dataset\n",
    "    src  = torch.randint(0, 10000, (1, DE_SEQ_LEN)).long()#.cuda()\n",
    "    trg = torch.randint(0, 10000, (1, EN_SEQ_LEN)).long()#.cuda()\n",
    "    ###\n",
    "\n",
    "    encoder_engine.train()\n",
    "    decoder_engine.train()\n",
    "    src = src.to(\"cpu\")\n",
    "    trg = trg.to(\"cpu\")\n",
    "    \n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        enc_keys = encoder_engine(src)\n",
    "        loss = decoder_engine(trg, keys=enc_keys, return_loss=True)\n",
    "        loss.sum().backward()\n",
    "\n",
    "    print(f'training loss: {loss.sum().item()}')\n",
    "    torch.nn.utils.clip_grad_norm_(encoder_engine.parameters(), 0.5)\n",
    "    torch.nn.utils.clip_grad_norm_(decoder_engine.parameters(), 0.5)\n",
    "    \n",
    "    enc_optim.step()\n",
    "    dec_optim.step()\n",
    "    \n",
    "    enc_optim.zero_grad()\n",
    "    dec_optim.zero_grad()\n",
    "\n",
    "    if tr_step % validate_every == 0:\n",
    "        encoder_engine.eval()\n",
    "        decoder_engine.eval()\n",
    "        with torch.no_grad():\n",
    "            ts_src = torch.randint(0, 10000, (1, DE_SEQ_LEN)).long()#.cuda()\n",
    "            ts_trg = torch.randint(0, 10000, (1, EN_SEQ_LEN)).long()#.cuda()\n",
    "\n",
    "            ts_src = ts_src.to(\"cpu\")\n",
    "            ts_trg = ts_trg.to(\"cpu\")\n",
    "\n",
    "            enc_keys = encoder_engine(ts_src)\n",
    "            loss = decoder_engine(ts_trg,\n",
    "                                  keys=enc_keys,\n",
    "                                  return_loss=True)\n",
    "                \n",
    "\n",
    "        print(\n",
    "            f'\\tValidation Loss: {loss.sum().item()}'\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5925f28f-24ca-491d-8561-1a247d43033e",
   "metadata": {},
   "source": [
    "## Translation 2\n",
    "Sequence to Sequence - Shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce8e0657-6698-4550-8115-ae8c7ef5693d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "DE_SEQ_LEN = 2048\n",
    "EN_SEQ_LEN = 2048\n",
    "\n",
    "enc_dec = ReformerEncDec(\n",
    "    dim = 256,\n",
    "    enc_num_tokens = 10000,\n",
    "    enc_depth = 6,\n",
    "    enc_max_seq_len = DE_SEQ_LEN,\n",
    "    dec_num_tokens = 10000,\n",
    "    dec_depth = 6,\n",
    "    dec_max_seq_len = EN_SEQ_LEN\n",
    ")#.cuda()\n",
    "\n",
    "optim = torch.optim.Adam(enc_dec.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "validate_every = 100\n",
    "for tr_step in tqdm.tqdm(range(1000)):\n",
    "    train_seq_in = torch.randint(0, 10000, (1, DE_SEQ_LEN)).long()#.cuda()\n",
    "    train_seq_out = torch.randint(0, 10000, (1, EN_SEQ_LEN)).long()#.cuda()\n",
    "    input_mask = torch.ones(1, DE_SEQ_LEN).bool()#.cuda()\n",
    "\n",
    "    loss = enc_dec(train_seq_in, train_seq_out, return_loss = True, enc_input_mask = input_mask)\n",
    "    loss.sum().backward()\n",
    "    # learn\n",
    "    torch.nn.utils.clip_grad_norm_(enc_dec.parameters(), 0.5)\n",
    "   \n",
    "    optim.step()    \n",
    "    optim.zero_grad()\n",
    "\n",
    "    # evaluate with the following\n",
    "    if tr_step % validate_every == 0:\n",
    "        eval_seq_in = torch.randint(0, 10000, (1, DE_SEQ_LEN)).long()#.cuda()\n",
    "        eval_seq_out_start = torch.tensor([[0.]]).long()#.cuda() \n",
    "        samples = enc_dec.generate(eval_seq_in, eval_seq_out_start, seq_len = EN_SEQ_LEN, eos_token = 1) \n",
    "        print(samples.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1bdd02-ed18-4d38-a5ae-cc735ad9039e",
   "metadata": {},
   "source": [
    "### Translation con dati reali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0ee44a-cbc6-4f9d-a169-75980fa8355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_en_file = \"https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.en\"\n",
    "tr_de_file = \"https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.de\"\n",
    "\n",
    "ts_en_file = \"https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/newstest2015.en\"\n",
    "ts_de_file = \"https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/newstest2015.de\"\n",
    "\n",
    "if not os.path.exists(\"./train.en\"):\n",
    "    !wget $tr_en_file\n",
    "if not os.path.exists(\"./train.de\"):\n",
    "    !wget $tr_de_file\n",
    "if not os.path.exists(\"./newstest2015.en\"):\n",
    "    !wget $ts_en_file\n",
    "if not os.path.exists(\"./newstest2015.de\"):\n",
    "    !wget $ts_de_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36ed6048-e7bd-4837-a7c6-74cf7a161d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4468840/4468840 [01:04<00:00, 68898.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totale vocabulary en: 2805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4468840/4468840 [01:13<00:00, 60663.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totale vocabulary de: 2505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def construct_vocab(dataset):\n",
    "    char_to_id = {\n",
    "        \"SOS\": 0,\n",
    "        \"EOS\": 1,\n",
    "        \"PAD\": 2\n",
    "    }\n",
    "    id_to_char = {\n",
    "        0: \"SOS\",\n",
    "        1: \"EOS\",\n",
    "        2: \"PAD\"\n",
    "    }\n",
    "    for line in tqdm.tqdm(dataset):\n",
    "        for tk in line.lower():\n",
    "            idx = len(char_to_id.keys())\n",
    "            if tk not in char_to_id:\n",
    "                char_to_id[tk] = idx\n",
    "                id_to_char[idx] = tk\n",
    "    return char_to_id, id_to_char\n",
    "\n",
    "train_en = open(\"./train.en\").readlines()\n",
    "char_to_id_en, id_to_char_en = construct_vocab(train_en)\n",
    "print(f\"Totale vocabulary en: {len(char_to_id_en)}\")\n",
    "\n",
    "train_de = open(\"./train.de\").readlines()\n",
    "char_to_id_de, id_to_char_de = construct_vocab(train_de)\n",
    "print(f\"Totale vocabulary de: {len(char_to_id_de)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "041479c4-00e1-4367-a56a-ce4234bb93c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_SEQ_LEN = 2048\n",
    "EN_SEQ_LEN = 2048\n",
    "\n",
    "enc_dec = ReformerEncDec(\n",
    "    dim = 256,\n",
    "    enc_num_tokens = len(char_to_id_de),\n",
    "    enc_depth = 6,\n",
    "    enc_max_seq_len = DE_SEQ_LEN,\n",
    "    dec_num_tokens = len(char_to_id_en),\n",
    "    dec_depth = 6,\n",
    "    dec_max_seq_len = EN_SEQ_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3803a751-620d-47e5-92b1-35c1e6442f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(enc_dec.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "455de9ec-927a-49fc-96bc-ab60a640d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor(sentence, char_to_id):\n",
    "    tokens = [char_to_id[\"SOS\"]]\n",
    "    for char in sentence:\n",
    "        if len(tokens) < (SEQ_LEN-1):\n",
    "            if char in char_to_id:\n",
    "                tokens.append(char_to_id[char])\n",
    "    tokens.append(char_to_id[\"EOS\"])\n",
    "\n",
    "    while len(tokens) < SEQ_LEN:\n",
    "        tokens.append(char_to_id[\"PAD\"])\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c5309d-acc6-4903-bf71-caac64013fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[SOS] the cat is on the table     [EOS] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
    "[SOS] il gatto è sul tavolo [EOS] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a9d973c-0d94-4d63-b080-0caef9427340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_sentece():\n",
    "    en_sentences = open(\"./newstest2015.en\").readlines()\n",
    "    de_sentences = open(\"./newstest2015.de\").readlines()\n",
    "    assert len(en_sentences) == len(de_sentences)\n",
    "    \n",
    "    sent_idx = random.randint(0, len(en_sentences))\n",
    "    \n",
    "    return en_sentences[sent_idx].lower(), de_sentences[sent_idx].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "811ae360-ae46-4ec6-af96-638e26cc2ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                                  | 0/1117210 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 7.1914496421813965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                  | 1/1117210 [08:05<150639:53:34, 485.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 141])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                   | 2/1117210 [11:18<97232:34:59, 313.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 5.719971179962158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                   | 3/1117210 [14:30<80039:55:56, 257.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.518599987030029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                   | 4/1117210 [17:23<69662:10:23, 224.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.384230136871338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                   | 5/1117210 [20:19<64190:19:15, 206.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.572434186935425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                   | 6/1117210 [23:17<61191:32:02, 197.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.978898525238037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                   | 7/1117210 [26:13<59028:34:06, 190.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.3155440092086792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                   | 7/1117210 [29:17<77894:50:40, 251.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.148115873336792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validate_every = 100    \n",
    "for tr_step in tqdm.tqdm(range(0,len(train_en), BATCH_SIZE), mininterval=10., desc='training'):\n",
    "    en_input = []\n",
    "    de_input = []\n",
    "    for j in range(BATCH_SIZE):\n",
    "        en_sentence = train_en[tr_step+j].lower()\n",
    "        en_tokens = create_tensor(sentence=en_sentence, char_to_id=char_to_id_en)\n",
    "        en_input.append(en_tokens)\n",
    "        \n",
    "        de_sentence = train_de[tr_step+j].lower()\n",
    "        de_tokens = create_tensor(sentence=de_sentence, char_to_id=char_to_id_de)\n",
    "        de_input.append(de_tokens)\n",
    "        \n",
    "    en_input = torch.tensor(en_input)\n",
    "    de_input = torch.tensor(de_input)\n",
    "    \n",
    "    \n",
    "    input_mask = torch.ones(BATCH_SIZE, DE_SEQ_LEN).bool()#.cuda()\n",
    "\n",
    "    loss = enc_dec(de_input, en_input, return_loss = True, enc_input_mask = input_mask)\n",
    "    loss.sum().backward()\n",
    "    print(f\"Training loss: {loss.sum().item()}\")\n",
    "    # learn\n",
    "    torch.nn.utils.clip_grad_norm_(enc_dec.parameters(), 0.5)\n",
    "   \n",
    "    optim.step()    \n",
    "    optim.zero_grad()\n",
    "\n",
    "    # evaluate with the following\n",
    "    if tr_step % validate_every == 0:\n",
    "        en_ts_sent, de_ts_sent = get_eval_sentece()\n",
    "        \n",
    "        en_ts_tokens = create_tensor(sentence=en_ts_sent, char_to_id=char_to_id_en)\n",
    "        de_ts_tokens = create_tensor(sentence=de_ts_sent, char_to_id=char_to_id_de)\n",
    "        \n",
    "        eval_seq_de = torch.tensor([de_ts_tokens])\n",
    "        eval_seq_en_start = torch.tensor([[char_to_id_en[\"SOS\"]]]).long()#.cuda() \n",
    "        samples = enc_dec.generate(eval_seq_de, eval_seq_en_start, seq_len = EN_SEQ_LEN, eos_token = char_to_id_en[\"EOS\"]) \n",
    "        print(samples.shape)\n",
    "    \n",
    "    if tr_step > 24:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9aa7a3-e83d-4a9f-9446-eb4745bb8ae8",
   "metadata": {},
   "source": [
    "## Facciamo una evaluation di prova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3454a513-2904-4ba5-81ca-683f86b0bd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "en_ts_sent, de_ts_sent = get_eval_sentece()\n",
    "\n",
    "en_ts_tokens = create_tensor(sentence=en_ts_sent, char_to_id=char_to_id_en)\n",
    "de_ts_tokens = create_tensor(sentence=de_ts_sent, char_to_id=char_to_id_de)\n",
    "\n",
    "eval_seq_de = torch.tensor([de_ts_tokens])\n",
    "eval_seq_en_start = torch.tensor([[char_to_id_en[\"SOS\"]]]).long()#.cuda() \n",
    "samples = enc_dec.generate(eval_seq_de, eval_seq_en_start, seq_len = EN_SEQ_LEN, eos_token = char_to_id_en[\"EOS\"]) \n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f878f4-a8c2-4f91-a8d5-2704dc8556c0",
   "metadata": {},
   "source": [
    "## Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474cbf1-e71f-48f2-b4b8-22e4ac128ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential\n",
    "from torchvision import models\n",
    "from reformer_pytorch import Reformer, ReformerLM\n",
    "\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = Sequential(*list(resnet.children())[:-4])\n",
    "\n",
    "SEQ_LEN = 4096\n",
    "\n",
    "encoder = Reformer(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    max_seq_len = 4096\n",
    ")\n",
    "\n",
    "decoder = ReformerLM(\n",
    "    num_tokens = 20000,\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    causal = True\n",
    ")\n",
    "\n",
    "x  = torch.randn(1, 3, 512, 512)\n",
    "yi = torch.randint(0, 20000, (1, SEQ_LEN)).long()\n",
    "\n",
    "visual_emb = resnet(x)\n",
    "b, c, h, w = visual_emb.shape\n",
    "visual_emb = visual_emb.view(1, c, h * w).transpose(1, 2) # nchw to nte\n",
    "\n",
    "enc_keys = encoder(visual_emb)\n",
    "yo = decoder(yi, keys = enc_keys) # (1, 4096, 20000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
