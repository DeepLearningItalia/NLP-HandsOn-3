{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb15c95-bf6f-4c1d-80eb-216de4b11f98",
   "metadata": {},
   "source": [
    "### Sinkhorn Transformer\n",
    "\n",
    "Dopo aver installato le dipendenze con: \n",
    "- conda create -n nlp-hands-on-3 python=3.8\n",
    "- pip install -r requirements.txt\n",
    "\n",
    "Vediamo in azione il nostro Sinkhorn Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ff6c7f-82e9-426c-b018-2cdffe0477eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "from sinkhorn_transformer import SinkhornTransformerLM\n",
    "from sinkhorn_transformer import Autopadder\n",
    "from sinkhorn_transformer.autoregressive_wrapper import AutoregressiveWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c68b7f8c-76f9-427e-9d32-9ed8c0c1c93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1955, -0.4047, -0.3095,  ..., -0.7498, -1.0335,  0.2164],\n",
       "         [-0.1427, -0.6244, -0.4162,  ..., -1.0227, -1.0701,  1.0770],\n",
       "         [ 0.1593,  0.1160, -0.0282,  ..., -0.2226, -0.8753,  0.9878],\n",
       "         ...,\n",
       "         [ 0.0035, -0.5261,  0.9306,  ...,  0.5631, -0.1365,  0.3768],\n",
       "         [ 0.2647, -1.1849,  0.6168,  ...,  0.3333, -0.5049,  0.2688],\n",
       "         [ 0.0386, -0.7799,  0.5175,  ...,  0.2789,  0.7087, -0.3994]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = SinkhornTransformerLM(\n",
    "    num_tokens = 20000,\n",
    "    dim = 1024,\n",
    "    heads = 8,\n",
    "    depth = 12,\n",
    "    max_seq_len = 8192,\n",
    "    bucket_size = 128,        # size of the buckets\n",
    "    causal = False,           # auto-regressive or not\n",
    "    n_sortcut = 2,            # use sortcut to reduce memory complexity to linear\n",
    "    n_top_buckets = 2,        # sort specified number of key/value buckets to one query bucket. paper is at 1, defaults to 2\n",
    "    ff_chunks = 10,           # feedforward chunking, from Reformer paper\n",
    "    reversible = True,        # make network reversible, from Reformer paper\n",
    "    emb_dropout = 0.1,        # embedding dropout\n",
    "    ff_dropout = 0.1,         # feedforward dropout\n",
    "    attn_dropout = 0.1,       # post attention dropout\n",
    "    attn_layer_dropout = 0.1, # post attention layer dropout\n",
    "    layer_dropout = 0.1,      # add layer dropout, from 'Reducing Transformer Depth on Demand' paper\n",
    "    weight_tie = True,        # tie layer parameters, from Albert paper\n",
    "    emb_dim = 128,            # embedding factorization, from Albert paper\n",
    "    dim_head = 64,            # be able to fix the dimension of each head, making it independent of the embedding dimension and the number of heads\n",
    "    ff_glu = True,            # use GLU in feedforward, from paper 'GLU Variants Improve Transformer'\n",
    "    n_local_attn_heads = 2,   # replace N heads with local attention, suggested to work well from Routing Transformer paper\n",
    "    pkm_layers = (4,7),       # specify layers to use product key memory. paper shows 1 or 2 modules near the middle of the transformer is best\n",
    "    pkm_num_keys = 128,       # defaults to 128, but can be increased to 256 or 512 as memory allows\n",
    ")\n",
    "\n",
    "x = torch.randint(0, 20000, (1, 2048))\n",
    "model(x) # (1, 2048, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e9de39-b586-4ca9-81ea-1ec5ceb1fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_en_file = \"https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.en\"\n",
    "tr_de_file = \"https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.de\"\n",
    "\n",
    "ts_en_file = \"https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/newstest2015.en\"\n",
    "ts_de_file = \"https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/newstest2015.de\"\n",
    "\n",
    "if not os.path.exists(\"./train.en\"):\n",
    "    !wget $tr_en_file\n",
    "if not os.path.exists(\"./train.de\"):\n",
    "    !wget $tr_de_file\n",
    "if not os.path.exists(\"./newstest2015.en\"):\n",
    "    !wget $ts_en_file\n",
    "if not os.path.exists(\"./newstest2015.de\"):\n",
    "    !wget $ts_de_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b555151e-8ab4-4fbc-b386-aede321eb479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4468840/4468840 [01:05<00:00, 68303.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totale vocabulary en: 2805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4468840/4468840 [01:14<00:00, 59694.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totale vocabulary de: 2505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def construct_vocab(dataset):\n",
    "    char_to_id = {\n",
    "        \"SOS\": 0,\n",
    "        \"EOS\": 1,\n",
    "        \"PAD\": 2\n",
    "    }\n",
    "    id_to_char = {\n",
    "        0: \"SOS\",\n",
    "        1: \"EOS\",\n",
    "        2: \"PAD\"\n",
    "    }\n",
    "    for line in tqdm.tqdm(dataset):\n",
    "        for tk in line.lower():\n",
    "            idx = len(char_to_id.keys())\n",
    "            if tk not in char_to_id:\n",
    "                char_to_id[tk] = idx\n",
    "                id_to_char[idx] = tk\n",
    "    return char_to_id, id_to_char\n",
    "\n",
    "train_en = open(\"./train.en\").readlines()\n",
    "char_to_id_en, id_to_char_en = construct_vocab(train_en)\n",
    "print(f\"Totale vocabulary en: {len(char_to_id_en)}\")\n",
    "\n",
    "train_de = open(\"./train.de\").readlines()\n",
    "char_to_id_de, id_to_char_de = construct_vocab(train_de)\n",
    "print(f\"Totale vocabulary de: {len(char_to_id_de)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff4b5e5b-5025-42dc-921e-ef008d60cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 4096\n",
    "SEQ_LEN = 4096\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "VALIDATE_EVERY  = 100\n",
    "GENERATE_EVERY  = 500\n",
    "GENERATE_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888b1dce-5366-4694-a243-9d54ca4ed94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = SinkhornTransformerLM(\n",
    "    num_tokens = len(char_to_id_en),\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    bucket_size = 128,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    reversible = True,\n",
    "    return_embeddings = True\n",
    ")#.cuda()\n",
    "\n",
    "enc = AutoregressiveWrapper(enc)\n",
    "\n",
    "#enc = Autopadder(enc, pad_left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb68aade-c385-4ba6-b81f-e0ad54a84c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = SinkhornTransformerLM(\n",
    "    num_tokens = len(char_to_id_de),\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    causal = True,\n",
    "    bucket_size = 128,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    receives_context = True,\n",
    "    context_bucket_size = 128,  # context key / values can be bucketed differently\n",
    "    reversible = True\n",
    ")#.cuda()\n",
    "\n",
    "dec = AutoregressiveWrapper(dec)\n",
    "\n",
    "#dec = Autopadder(dec, pad_left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dfe0f49-b7b5-445f-b31e-77a1309da24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_optim = torch.optim.Adam(enc.parameters(), lr=LEARNING_RATE)\n",
    "dec_optim = torch.optim.Adam(dec.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6e81078-b92f-46a0-85e6-9f351c278609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor(sentence, char_to_id):\n",
    "    tokens = [char_to_id[\"SOS\"]]\n",
    "    for char in sentence:\n",
    "        if len(tokens) < (SEQ_LEN-1):\n",
    "            tokens.append(char_to_id[char])\n",
    "    tokens.append(char_to_id[\"EOS\"])\n",
    "\n",
    "    while len(tokens) < SEQ_LEN:\n",
    "        tokens.append(char_to_id[\"PAD\"])\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d4858c2-e16d-41c4-b686-4dea583ef254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                   | 1/1117210 [02:49<52664:25:57, 169.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 7.434681415557861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                   | 2/1117210 [05:34<51709:35:15, 166.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 5.121203422546387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                   | 3/1117210 [07:47<47044:15:45, 151.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.3080813884735107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                   | 4/1117210 [10:01<44867:20:56, 144.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.6939918994903564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                   | 5/1117210 [11:49<40706:12:15, 131.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.8909008502960205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                   | 6/1117210 [13:33<37871:51:23, 122.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6237004399299622\n",
      "training loss: 0.25593942403793335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                   | 7/1117210 [15:35<37888:44:03, 122.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.3690425753593445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|                                                                                                                   | 7/1117210 [18:54<50301:56:57, 162.09s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(0,len(train_en), BATCH_SIZE), mininterval=10., desc='training'):\n",
    "    en_input = [] # Batch!\n",
    "    de_input = []\n",
    "    for j in range(BATCH_SIZE):\n",
    "        en_sentence = train_en[i+j].lower()\n",
    "        en_tokens = create_tensor(sentence=en_sentence, char_to_id=char_to_id_en)\n",
    "        en_input.append(en_tokens)\n",
    "        \n",
    "        de_sentence = train_de[i+j].lower()\n",
    "        de_tokens = create_tensor(sentence=de_sentence, char_to_id=char_to_id_de)\n",
    "        de_input.append(de_tokens)\n",
    "        \n",
    "    en_input = torch.tensor(en_input)\n",
    "    de_input = torch.tensor(de_input)\n",
    "    \n",
    "    en_mask = torch.ones_like(en_input).bool()#.cuda()\n",
    "    de_mask = torch.ones_like(de_input).bool()#.cuda()\n",
    "        \n",
    "    enc.train()\n",
    "    dec.train()\n",
    "    \n",
    "    context = enc(en_input, input_mask=en_mask)\n",
    "    loss = dec(de_input, context=context, input_mask=de_mask, context_mask=en_mask, return_loss = True) \n",
    "    loss.backward()\n",
    "\n",
    "    print(f'training loss: {loss.item()}')\n",
    "    torch.nn.utils.clip_grad_norm_(enc.parameters(), 0.5)\n",
    "    torch.nn.utils.clip_grad_norm_(dec.parameters(), 0.5)\n",
    "    enc_optim.step()\n",
    "    enc_optim.zero_grad()\n",
    "    dec_optim.step()\n",
    "    dec_optim.zero_grad()\n",
    "    \n",
    "    if i > 24:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da3bc3-e0e5-4f07-a485-4ad51194beaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
